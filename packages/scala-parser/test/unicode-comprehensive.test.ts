import { allTokens } from "../src/lexer.js";
import { parserInstance } from "../src/parser.js";
import {
  normalizeUnicode,
  isValidIdentifier,
  processUnicodeEscapes,
  MATHEMATICAL_SYMBOLS,
} from "../src/unicode-utils";
import { createToken, Lexer } from "chevrotain";
import { describe, it, expect } from "vitest";

const lexerInstance = new Lexer(allTokens);

describe("Comprehensive Unicode Identifier Support", () => {
  it("should support Greek letters", () => {
    const inputText = `val Î± = 42
val Î² = "beta"
val Î“ = "gamma"
val Î” = "delta"`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });

  it("should support Japanese characters (hiragana, katakana, kanji)", () => {
    const inputText = `val ã‚ã„ã†ãˆãŠ = "hiragana"
val ã‚¢ã‚¤ã‚¦ã‚¨ã‚ª = "katakana"
val å¤‰æ•°å = "kanji"
val æ•°å€¤ = 42`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });

  it("should support Chinese characters", () => {
    const inputText = `val å˜é‡ = "simplified chinese"
val è®Šé‡ = "traditional chinese"
val ä¸­æ–‡æ ‡è¯†ç¬¦ = "chinese identifier"`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });

  it("should support Cyrillic (Russian) characters", () => {
    const inputText = `val Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ = "russian"
val ÐŸÐ•Ð Ð•ÐœÐ•ÐÐÐÐ¯ = "russian uppercase"
val Ñ€Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¸Ð¹_Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€ = "russian identifier"`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });

  it("should support Arabic characters", () => {
    const inputText = `val Ù…ØªØºÙŠØ± = "arabic"
val Ø§Ù„Ù…ØªØºÙŠØ±_Ø§Ù„Ø¹Ø±Ø¨ÙŠ = "arabic identifier"`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });

  it("should support Hebrew characters", () => {
    const inputText = `val ×ž×©×ª× ×” = "hebrew"
val ×”×ž×©×ª× ×”_×”×¢×‘×¨×™ = "hebrew identifier"`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });

  it("should support accented Latin characters", () => {
    const inputText = `val cafÃ© = "with accent"
val naÃ¯ve = "with diaeresis" 
val rÃ©sumÃ© = "with acute"
val piÃ±ata = "with tilde"`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });

  it("should support mathematical symbols", () => {
    const inputText = `val âˆ‘ = "summation"
val âˆ = "product"
val âˆ‚ = "partial derivative"
val âˆ‡ = "nabla"
val âˆž = "infinity"`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });

  it("should support emoji identifiers", () => {
    const inputText = `val ðŸš€ = "rocket"
val ðŸ“ = "memo"
val ðŸŒŸ = "star"
val ðŸ’» = "computer"`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });

  it("should support mixed Unicode identifiers", () => {
    const inputText = `val mixed_å¤‰æ•°_Î±_ðŸŒŸ = "mixed identifiers"
val cafÃ©_ä¸­æ–‡_Ñ€ÑƒÑÑÐºÐ¸Ð¹ = "multilingual"
val test_æ•°å€¤_âˆ‘_emojiðŸŽ¯ = "comprehensive test"`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });

  it("should support special characters in identifiers", () => {
    const inputText = `val _private = "underscore prefix"
val test_ = "underscore suffix"
val camelCase = "standard identifier"
val MixedCase123 = "mixed case with numbers"`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });

  it("should handle Unicode in function parameters and class definitions", () => {
    const inputText = `def è¨ˆç®—(æ•°å€¤Î±: Int, æ•°å€¤Î²: Int): Int = æ•°å€¤Î± + æ•°å€¤Î²

class å®¹å™¨[åž‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿](å€¤: åž‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) {
  def å–å¾—: åž‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ = å€¤
}

case class ç‚¹(Ï‡åº§æ¨™: Double, Ïˆåº§æ¨™: Double)`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });
});

describe("Unicode Property Classes Enhancement", () => {
  it("should handle Unicode escapes in string literals", () => {
    const inputText = `val greeting = "Hello \\u03B1\\u03B2\\u03B3"  // Î± Î² Î³
val symbol = "\\u2200 x \\u2203 y"  // âˆ€ x âˆƒ y
val arrow = "f: A \\u2192 B"  // f: A â†’ B`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });

  it("should handle Unicode escapes in char literals", () => {
    const inputText = `val alpha = '\\u03B1'  // Î±
val lambda = '\\u03BB'  // Î»
val pi = '\\u03C0'  // Ï€`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });

  it("should support advanced mathematical symbols", () => {
    const inputText = `val âˆ«integral = 42
val Î”delta = 0.01
val âˆ‘sum = 100
val âˆ€forall = true
val âˆƒexists = false`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });

  it("should handle functional programming with Greek letters", () => {
    const inputText = `val Î± = 1
val Î² = 2
val Î¼ = 3
val Î» = 4
val Ï€ = 3.14159`;

    const lexingResult = lexerInstance.tokenize(inputText);
    expect(lexingResult.errors).toHaveLength(0);

    parserInstance.input = lexingResult.tokens;
    const cst = parserInstance.compilationUnit();
    expect(parserInstance.errors).toHaveLength(0);
  });
});

describe("Unicode Utility Functions", () => {
  it("should normalize Unicode strings correctly", () => {
    // Test with composed vs decomposed characters
    const composed = "cafÃ©"; // Ã© as single character
    const decomposed = "cafÃ©"; // e + combining acute accent

    const normalizedComposed = normalizeUnicode(composed);
    const normalizedDecomposed = normalizeUnicode(decomposed);

    expect(normalizedComposed).toBe(normalizedDecomposed);
  });

  it("should validate Unicode identifiers", () => {
    expect(isValidIdentifier("Î»")).toBe(true);
    expect(isValidIdentifier("Î±123")).toBe(true);
    expect(isValidIdentifier("âˆ€universal")).toBe(true);
    expect(isValidIdentifier("å¤‰æ•°")).toBe(true);
    expect(isValidIdentifier("_underscore")).toBe(true);
    expect(isValidIdentifier("$dollar")).toBe(true);

    // Invalid identifiers
    expect(isValidIdentifier("123invalid")).toBe(false);
    expect(isValidIdentifier("")).toBe(false);
    expect(isValidIdentifier("@invalid")).toBe(false);
  });

  it("should process Unicode escapes", () => {
    const input = "Hello \\u03B1\\u03B2\\u03B3";
    const processed = processUnicodeEscapes(input);
    expect(processed).toBe("Hello Î±Î²Î³");

    const mathematicalInput = "\\u2200 x \\u2203 y";
    const mathematicalProcessed = processUnicodeEscapes(mathematicalInput);
    expect(mathematicalProcessed).toBe("âˆ€ x âˆƒ y");
  });

  it("should provide mathematical symbol constants", () => {
    expect(MATHEMATICAL_SYMBOLS.ALPHA).toBe("Î±");
    expect(MATHEMATICAL_SYMBOLS.BETA).toBe("Î²");
    expect(MATHEMATICAL_SYMBOLS.GAMMA).toBe("Î³");
    expect(MATHEMATICAL_SYMBOLS.LAMBDA).toBe("Î»");
    expect(MATHEMATICAL_SYMBOLS.PI).toBe("Ï€");
    expect(MATHEMATICAL_SYMBOLS.FORALL).toBe("âˆ€");
    expect(MATHEMATICAL_SYMBOLS.EXISTS).toBe("âˆƒ");
  });
});
